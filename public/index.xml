<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hidden Layer</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Hidden Layer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 07 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>First Steps on Kaggle: a friendly guide to getting started with Practical AI</title>
      <link>http://localhost:1313/kaggle/first-steps-on-kaggle/</link>
      <pubDate>Wed, 07 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/kaggle/first-steps-on-kaggle/</guid>
      <description>
        
          
            &lt;figure class=&#34;blog-figure&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/kaggle-logo.png&#34; alt=&#34;Kaggle Logo&#34; class=&#34;blog-image&#34; loading=&#34;lazy&#34;&gt;
    
&lt;/figure&gt;

&lt;p&gt;Welcome to our &lt;strong&gt;Kaggle journey&lt;/strong&gt;! If you&amp;rsquo;re curious about AI and machine learning but feel a bit overwhelmed, you&amp;rsquo;re in the right place. Think of Kaggle as your friendly neighborhood playground for data science: a place where you can learn, experiment, and grow alongside a supportive community.&lt;/p&gt;
&lt;h2 id=&#34;what-is-kaggle&#34;&gt;What is Kaggle?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/&#34;&gt;&lt;strong&gt;Kaggle&lt;/strong&gt;&lt;/a&gt; is an online platform owned by Google that brings together data enthusiasts, AI engineers, and curious learners from around the world. It offers:&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Attention is All You Need: The Paper that Changed AI</title>
      <link>http://localhost:1313/papers/attention-is-all-you-need/</link>
      <pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/papers/attention-is-all-you-need/</guid>
      <description>
        
          
            &lt;h1 id=&#34;attention-is-all-you-need&#34;&gt;Attention is All You Need&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Ashish Vaswani, Noam Shazeer, Niki Parmar et al.&lt;br&gt;
&lt;strong&gt;Publication Date&lt;/strong&gt;: June 2017&lt;br&gt;
&lt;strong&gt;Published In&lt;/strong&gt;: NeurIPS 2017&lt;br&gt;
&lt;strong&gt;Number of Citations&lt;/strong&gt;: 80,000+&lt;br&gt;
&lt;strong&gt;Link to Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;arXiv.org/attention&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#brief-description&#34;&gt;Brief Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#main-topic&#34;&gt;Main Topic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#key-concepts&#34;&gt;Key Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#1-self-attention-mechanism&#34;&gt;1. Self-Attention Mechanism&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#2-multi-head-attention&#34;&gt;2. Multi-Head Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#3-position-wise-feed-forward-networks&#34;&gt;3. Position-wise Feed-Forward Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#4-positional-encoding&#34;&gt;4. Positional Encoding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#methodology-and-experiments&#34;&gt;Methodology and Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#results-and-practical-impact&#34;&gt;Results and Practical Impact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#references-and-related-work&#34;&gt;References and Related Work&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#core-references-cited-by-authors&#34;&gt;Core References&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#subsequent-influential-work-inspired-by-this-paper&#34;&gt;Subsequent Influential Work&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#conclusion-and-takeaways&#34;&gt;Conclusion and Takeaways&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#key-takeaways&#34;&gt;Key Takeaways&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#why-this-paper-still-matters&#34;&gt;Why This Paper Still Matters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/papers/attention-is-all-you-need/#thanks-for-reading&#34;&gt;Thanks for Reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;brief-description&#34;&gt;Brief Description&lt;/h2&gt;
&lt;p&gt;This seminal paper introduced the Transformer architecture, which relies entirely on attention mechanisms, revolutionizing natural language processing and laying the foundation for modern generative AI. The architecture has become the backbone of models like GPT, BERT, and their successors.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>AI Weekly ‚Äì May 5, 2025</title>
      <link>http://localhost:1313/ai-llm-news/ai-weekly-may-5-2025/</link>
      <pubDate>Mon, 05 May 2025 15:24:18 -0300</pubDate>
      
      <guid>http://localhost:1313/ai-llm-news/ai-weekly-may-5-2025/</guid>
      <description>
        
          
            &lt;p&gt;This week brings pivotal advancements in the AI landscape, from redefining developer roles to enhancing search experiences and bolstering national security.&lt;/p&gt;
&lt;h2 id=&#34;1-developers-evolve-into-builders-in-the-ai-era&#34;&gt;1. Developers Evolve into &amp;lsquo;Builders&amp;rsquo; in the AI Era&lt;/h2&gt;




&lt;figure class=&#34;blog-figure&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/ai-weekly-may-5-2025_01.png&#34; alt=&#34;Developers evolving into builders in the AI era&#34; class=&#34;blog-image&#34; loading=&#34;lazy&#34;&gt;
    
&lt;/figure&gt;

&lt;p&gt;Varun Mohan, CEO of Windsurf (formerly Codeium), envisions a transformative shift where traditional software developers become &amp;ldquo;builders.&amp;rdquo; This change is driven by AI&amp;rsquo;s capability to democratize software creation, allowing individuals to customize tools using AI assistants, even without formal coding knowledge. Windsurf, an AI coding platform, has gained significant traction, raising $243 million and potentially being acquired by OpenAI for around $3 billion. Mohan emphasizes the rise of &amp;ldquo;vibe coding,&amp;rdquo; a method of generating code via AI prompts that redefines development processes by vastly improving speed and productivity. This evolution encourages a culture of continuous learning and innovation fueled by AI capabilities.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>What People Think AI Engineers Do (vs Reality)</title>
      <link>http://localhost:1313/become-an-ai-expert/ai-engineer-meme_01/</link>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/become-an-ai-expert/ai-engineer-meme_01/</guid>
      <description>
        
          
            &lt;h1 id=&#34;what-people-think-ai-engineers-do-vs-reality&#34;&gt;What People Think AI Engineers Do (vs Reality)&lt;/h1&gt;




&lt;figure class=&#34;blog-figure&#34;&gt;
    &lt;img src=&#34;http://localhost:1313/images/ai_meme_01.png&#34; alt=&#34;AI Engineer Meme&#34; class=&#34;blog-image&#34; loading=&#34;lazy&#34;&gt;
    
&lt;/figure&gt;

&lt;p&gt;We‚Äôve all seen those memes that contrast ‚ÄúWhat people think I do vs What I really do.‚Äù&lt;br&gt;
So I made one for the &lt;strong&gt;AI Engineer&lt;/strong&gt; role, because honestly, the misconceptions are getting out of hand.&lt;/p&gt;
&lt;p&gt;Let‚Äôs break them down:&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-my-coworkers-think-i-do&#34;&gt;üß† What my coworkers think I do&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;‚ÄúTalk to ChatGPT all day.‚Äù&lt;/em&gt;&lt;br&gt;
To be fair, this happens. But it‚Äôs not the core of the job. Prompting is not engineering.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Welcome to HiddenLayer</title>
      <link>http://localhost:1313/become-an-ai-expert/welcome/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/become-an-ai-expert/welcome/</guid>
      <description>
        
          
            &lt;h1 id=&#34;welcome-to-hiddenlayer&#34;&gt;Welcome to HiddenLayer&lt;/h1&gt;
&lt;p&gt;This is &lt;strong&gt;HiddenLayer&lt;/strong&gt; ‚Äî a space where I share, with total clarity and honesty, what it really means to work with Artificial Intelligence in 2025. Here, you won‚Äôt find LinkedIn buzzwords or empty hype. Instead, you‚Äôll find:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;My journey as an AI engineer&lt;/li&gt;
&lt;li&gt;Behind-the-scenes of real-world projects&lt;/li&gt;
&lt;li&gt;Insane bugs&lt;/li&gt;
&lt;li&gt;Tough architectural decisions&lt;/li&gt;
&lt;li&gt;Lessons learned through hands-on experience&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All of this from someone who‚Äôs spent years as a Data Scientist and chose to go deeper into the stack.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
